# Probes: Liveness and Readiness

Content:
    - livenessProbe
    - livenessProbe with exec      checking machanism
    - Probes config
    - livenessProbe with tcpSocket checking machanism
    - livenessProbe with httpGet   checking machanism
    - Observing of unhealthy Pod
    - readinessProbe and how it works
    - livenessProbe & readinessProbe in a single   Deployment

livenessProbe: Makes a periodical check to ensure the Pod is alive
    3 types of checks:
      - exec
      - httpGet
      - tcpSocket

# =================================================
# - livenessProbe with exec      checking machanism
1. Exec probe usage:  Create a Deployment that contains a livnessProbe check inside. 
#  See yaml below.
#  File: deploy-liveness-probe-exec.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: ubuntu
  labels:
    app: ubuntu
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ubuntu
  template:
    metadata:
      labels:
        app: ubuntu
    spec:
      containers:
      - name: ubuntu
        image: location/ubuntu
        args:  # Arguments - these commands will be executed inside the container on start:
        - /bin/bash 
        - -c        
        - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600
        ports:
        - containerPort: 8000
        livenessProbe:      # *********** LivenessProbe part: ***********
          exec:             # === exec type ======================================================
            command:        # Probe checks the exit-code of the command below.
            - cat           # - case, when exit-code == 0 is normal
            - /tmp/healthy  # - case, when exit-code <> 0 is NOT normal ---> Pod will be recreated.
          initialDelaySeconds: 5 # Defaults to 0 seconds. Minimum value is 0.
          periodSeconds:       5 # Defaults to 10 seconds. Minimum value is 1.
          timeoutSeconds:      1 # Defaults to 1 second. Minimum value is 1.
          successThreshold:    1 # Defaults to 1. Must be 1 for liveness and startup Probes. Min is 1
          failureThreshold:    3 # Defaults to 3. Minimum value is 1.
          
Start the above Deployment: kubectl apply -f deploy-liveness-probe-exec.yaml
Watch for EVENTs:           kubectl get events --watch
# In 30 seconds, after the /tmp/healthy is deleted,
# livenessProbe's exec will see that "cat /tmp/healthy" exited with code <> 0,
# This will make Kubernetes to consider the Pod as damaged,
# which will make it to kill the current Pod and restart it again.
# And so on forever - for this example.
Delete the above Deployment: kubectl delete -f deploy-liveness-probe-exec.yaml



# =================================================
# - livenessProbe with tcpSocket checking machanism
2. tcpSocket probe usage:  Create a Deployment that contains a livnessProbe check inside.
# Make change in example above, + service needed:
# deploy-liveness-probe-tcp.yaml
    .....
        livenessProbe:      # *********** LivenessProbe part: ***********
          tcpSocket:        # === TCP Socket type ===============================================
            port: 8001      # Probe periodically tries to connect to the specified port.
                            # - successful connection is OK
                            # - failure to connect will cause the Pod to be recreated
          initialDelaySeconds: 5 # Defaults to 0 seconds. Minimum value is 0.
          periodSeconds:       5 # Defaults to 10 seconds. Minimum value is 1.
          timeoutSeconds:      1 # Defaults to 1 second. Minimum value is 1.
          successThreshold:    1 # Defaults to 1. Must be 1 for liveness and startup Probes. Min is 1
          failureThreshold:    3 # Defaults to 3. Minimum value is 1.
---
apiVersion: v1
kind: Service
metadata:
  name: ubuntu-service-tcp
spec:
  selector:
    app: http-server-tcp
  ports:
    - protocol: TCP
      port:          80 # service port
      targetPort:  8000 # container port
      nodePort:   30002
  type: NodePort

Start the above Deployment: kubectl apply -f deploy-liveness-probe-tcp.yaml
Watch for EVENTs:           kubectl get events --watch
#   Kube will periodically try to connect to Pod's tcpSocket-port 8001 to check it liveness.
#   if it fails 'failureThreshold' times, the Pod will be restarted
Delete the above Deployment: kubectl delete -f deploy-liveness-probe-tcp.yaml



# =================================================
# - livenessProbe with httpGet   checking machanism
#
# - Observing of unhealthy Pod
#
3. httpGet probe usage:   Create a Deployment that contains a livnessProbe check inside.
# Make change in example above, + service needed:
# deploy-liveness-probe-tcp.yaml
Note: httpGet probe is made by sending http-get query to the container's IP:port
    - if the server responds with correct code, with no error      - that's   OK
    - if the server responds with ERROR   code, or doesn't respond - that's a FAULT

Create an image:
    - Within that image set a server that will send an ERRONEOUS response to specified port for some Kube's request.
    - Create a Deployment (with its Service) using that image
    - In that Deployment make a conf:
        # deploy-liveness-probe-http.yaml
        ....
        spec:
          template:
            spec:
              containers:
              - name: kuber-app
                image: location/kuber:v1.0-unhealthy
                ports:
                - containerPort: 8000
                # livenessProbe:
                #   httpGet:
                #     path: /healthcheck    # request will be sent to this path, in this ex.
                #     port: 8000
                #   initialDelaySeconds: 5  # waits 5 seconds after start
                #   periodSeconds: 5        # check periodically every 5 seconds
        ---
        apiVersion: v1
        kind: Service
        metadata:
        name: kuber-service-http
        spec:
        selector:
            app: http-server-tcp
        ports:
            - protocol: TCP
            port:          80 # service port
            targetPort:  8000 # container port
            nodePort:   30002 # <-- use this in browser for probe, see below in "Access"
        type: NodePort
        
Start the above Deployment: kubectl apply -f deploy-liveness-probe-http.yaml
Watch for EVENTs:           kubectl get events --watch

Access: <IP>:30002/healthcheck
#   if the response code is OK    (200, ...)      - then OK
#   if the response code is ERROR (500, 404, ...) - then FAULT
#       after 3 failures (default 'failureThreshold' is 3) the Container will be restarted
Watch for EVENTs:           kubectl get events --watch

Delete the above Deployment: kubectl delete -f deploy-liveness-probe-http.yaml


# =================================
# - readinessProbe and how it works
#
Note: readinessProbe is fully identical to livenessProbe, 
      all their configs are appliable to each other
      - readinessProbe is for knowing when to register the Pod to the Service and put into work
      - livenessProbe  is for making sure the Container is alive
      
#  File: deploy-readyness-probe-exec.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: kuber-http-redinessprobe
  labels:
    app: kuber
spec:
  replicas: 1
  selector:
    matchLabels:
      app: http-server-default
  template:
    metadata:
      labels:
        app: http-server-default
    spec:
      containers:
      - name: kuber-app
        image: location/kuber:v1.0-unhealthy
        ports:
        - containerPort: 8000
        readinessProbe:      # *********** readinessProbe part: ***********
          httpGet:             # === exec type ======================================================
            command:        # Probe checks the exit-code of the command below.
            - cat           # - case, when exit-code == 0 is normal
            - /tmp/healthy  # - case, when exit-code <> 0 is NOT normal ---> Pod will be recreated.
          initialDelaySeconds: 5 # Defaults to 0 seconds. Minimum value is 0.
          periodSeconds:       5 # Defaults to 10 seconds. Minimum value is 1.
          timeoutSeconds:      1 # Defaults to 1 second. Minimum value is 1.
          successThreshold:    1 # Defaults to 1. Must be 1 for liveness and startup Probes. Min is 1
          failureThreshold:    3 # Defaults to 3. Minimum value is 1.
         