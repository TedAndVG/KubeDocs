# ============ Kubernetes ============ 17June2024
Content:
    - Kubernetes Control Plane
    - Etcd
    - Kube Scheduler
    - ConfigMaps
    - Update Kubernetes
    - Update AWS EKS cluster


Kubernetes Control Plane:
    Kubernetes Control Plane, often called the “Master” or “Control Node,” is a set of components that collectively manage the state of a Kubernetes cluster.
    It acts as the brain of the cluster, making worldwide decisions about the cluster (for example, scheduling), 
     as well as detecting and responding to cluster events (like starting a new pod while a deployment’s replicas field is unsatisfied).
    Components of the Kubernetes Control Plane.
    There are mainly four main Control Plane components listed as follows:
        - Kube-API Server
        - Kube-Scheduler
        - Controller Manager
        - Etcd Database

Etcd:
    Etcd is a highly available key-value store for service discovery and shared configuration.
    The goal is to securely store important data within a distributed system via the Raft consensus system. 


Kube Scheduler:
    Kube Scheduler is in charge of assigning newly created pods to cluster nodes.
    It provides an efficient and equitable workload distribution through comparing the resource requirements of pods to the resources on nodes.
    To make these decisions, the scheduler considers an array of established policies and priorities, including node affinity, taints, and tolerance.
    It helps ensure maintaining the ideal cluster performance and resource usage by dynamically allocating pods.
    Administrators may modify scheduling behavior to suit specific application constraints with the aid of the Kube Scheduler’s integration with custom scheduling policies.
    - Assigns pods to nodes based on resource availability and constraints.
    - Regularly checks the resource utilization of every node and schedules pods to hold the desired state.
    - Enhances cluster efficiency by using distributing workloads intelligently.
    These components works collectively to maintain the desired state of the Kubernetes cluster, handle of events, and to make sure that programs are running as exactly like it was configured in user’s deployment configurations.

ConfigMaps:
    Configmap is an API object that is mainly used to store non-confidential data. 
    The data that is stored in ConfigMap is stored as key-value pairs.
    ConfigMaps are configuration files that may be used by pods as command-line arguments, environment variables, or even as configuration files on a disc.
    This feature allows us to decouple environment-specific configuration from our container images, after this is done, our applications are easily portable.
    The thing to be noted here is that ConfigMap does not provide any sort of secrecy or encryption, so it is advised to store non-confidential data only We can use secrets to store confidential data.
    ConfigMap has data and binary data fields. Key value pairs are accepted by these fields as values.
    The data field is used to store UTF-8 strings, while the binary data field is used to store binary data as base64-encoded strings.
    A valid DNS subdomain name should be given to ConfigMap.
    The key value that is recorded in the data field and the key value in the binaryData field cannot both be the same.


# =============================================================
# ==================== Upgrade Kubernetes =====================
# =============================================================
See:    https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/

    # =========== Add my user account on OS =================
        groupadd ted
        useradd -g ted -m -d /home/ted -s /bin/bash -c ted ted
        passwd ted
        
        vi /etc/sudoers.d/ted
        ted  ALL=(ALL)  NOPASSWD: ALL
        
        chmod 440 /etc/sudoers.d/ted
        # ==== Done. =====
        
# ============= Upgrading Kubernetes k8s ======================
    Prepare the Control and Worker nodes OS repos:
        Repo's file location - /etc/apt/sources.list.d/kubernetes.list
        - Add repo of needed version:
            echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.26/deb/ /' > /etc/apt/sources.list.d/kubernetes.list
            echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.27/deb/ /' >> /etc/apt/sources.list.d/kubernetes.list
            echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /' >> /etc/apt/sources.list.d/kubernetes.list
            echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /' >> /etc/apt/sources.list.d/kubernetes.list
            echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /' >> /etc/apt/sources.list.d/kubernetes.list
        - Register keyring .gpg:
            curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.26/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
        - Refresh repos:
            sudo apt update
            sudo apt-cache madison kubeadm
    Control node(s):
        Upgrade kubeadm on the Control Plane node
        Plan the upgrade (kubeadm upgrade plan)
        Apply the upgrade (kubeadm upgrade apply)
        Drain the Control Plane node (cordon)
        Upgrade kubelet & kubectl on the control Plane node
        Uncordon the Control Plane node
    Worker Nodes:
        Upgrade kubeadm on a Worker node
        Plan the upgrade (kubeadm upgrade plan)
        Apply the upgrade (kubeadm upgrade apply)
        Drain the Worker node (cordon)
        Upgrade kubelet & kubectl on the Worker Plane node
        Uncordon the Worker node
    
# =============================================================
# ================ Upgrading Control plane ====================

Upgrading control plane nodes:
    The upgrade procedure on control plane nodes should be executed one node at a time.
    Pick a control plane node that you wish to upgrade first. It must have the /etc/kubernetes/admin.conf file.
    Call "kubeadm upgrade": For the first control plane node
        1. Upgrade kubeadm:
            # replace x in 1.30.x-* with the latest patch version
sudo apt-mark unhold kubeadm && \
sudo apt-get update && sudo apt-get install -y kubeadm='1.29.6-1.1' && \
sudo apt-mark hold kubeadm
                    # some GUI, session dropped
        2. Verify that the download works and has the expected version:
            kubeadm version
    
        3. Verify the upgrade plan:
            sudo kubeadm upgrade plan
        
        4. Choose a version to upgrade to, and run the appropriate command. For example:
            # replace x with the patch version you picked for this upgrade
            sudo kubeadm upgrade apply v1.28.11


------------ Flannel --------------
https://github.com/flannel-io/flannel/issues/1390
        5. Manually upgrade your CNI provider plugin.
            Your Container Network Interface (CNI) provider may have its own upgrade instructions to follow. Check the addons page to find your CNI provider and see whether additional upgrade steps are required.
            This step is not required on additional control plane nodes if the CNI provider runs as a DaemonSet.
            name: kube-flannel
            image: docker.io/flannel/flannel:v0.22.0
        
            The procedure for the solo node:
                kubectl delete daemonset kube-flannel-ds --namespace=kube-system
                kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
                Restart host
                Hope for the best
    
            The procedure for the clustered:
                kubectl drain slave-node
                run steps from "solo node"-procedure but also deleting daemonsets without any pods
                image
                kubectl uncordon slave-node
            And all my deployments, pods, services etc are working.
--------- not done yet ---------

    For the other control plane nodes:
        Same as the first control plane node but use:
            sudo kubeadm upgrade node
        instead of:
            sudo kubeadm upgrade apply

        Also calling kubeadm upgrade plan and upgrading the CNI provider plugin is no longer needed.

    Drain the node:
        Prepare the node for maintenance by marking it unschedulable and evicting the workloads:
            # replace <node-to-drain> with the name of your node you are draining
            kubectl drain <node-to-drain> --ignore-daemonsets

    Upgrade kubelet and kubectl:
        1. Upgrade the kubelet and kubectl: # Ubuntu, Debian, CentOS, RHEL or Fedora
            # replace x in 1.27.x-* with the latest patch version
apt-mark unhold kubelet kubectl && \
apt-get update && apt-get install -y kubelet='1.29.6-1.1' kubectl='1.29.6-1.1' && \
apt-mark hold kubelet kubectl
            # node goes offline
        2. Restart the kubelet:
            sudo systemctl daemon-reload
            sudo systemctl restart kubelet

    Uncordon the node:
        Bring the node back online by marking it schedulable:
            # replace <node-to-uncordon> with the name of your node
            kubectl uncordon <node-to-uncordon>
                # node comes back online



# =============== Worker Nodes Upgrade ===============
# ====================================================

    # =========== Add my user account on OS =================
        groupadd ted
        useradd -g ted -m -d /home/ted -s /bin/bash -c ted ted
        passwd ted
        
        vi /etc/sudoers.d/ted
        ted  ALL=(ALL)  NOPASSWD: ALL
        
        chmod 440 /etc/sudoers.d/ted
        # ==== Done. =====
        
Upgrading worker nodes:
    After the kubelet is upgraded on all nodes verify that all nodes are available again by running the following command from anywhere kubectl can access the cluster
        kubectl get nodes
    (Consider topic: Recovering from a failure state)
    
    Upgrade kubeadm: #  Ubuntu, Debian, CentOS, RHEL or Fedora
        # replace x in 1.27.x-* with the latest patch version
apt-mark unhold kubeadm && \
apt-get update && apt-get install -y kubeadm='1.27.15-1.1' && \
apt-mark hold kubeadm

    Call "kubeadm upgrade":
        For worker nodes this upgrades the local kubelet configuration:
            sudo kubeadm upgrade node

    Drain the node: (on control plane)
        Prepare the node for maintenance by marking it unschedulable and evicting the workloads:
        # replace <node-to-drain> with the name of your node you are draining
            kubectl drain <node-to-drain> --ignore-daemonsets

    Upgrade kubelet and kubectl:
        1. Upgrade kubelet and kubectl: # Ubuntu, Debian, CentOS, RHEL or Fedora
            # replace x in 1.27.x-* with the latest patch version
apt-mark unhold kubelet kubectl && \
apt-get update && apt-get install -y kubelet='1.27.15-1.1' kubectl='1.27.15-1.1' && \
apt-mark hold kubelet kubectl
                # node goes offline

        2. Restart the kubelet:
            sudo systemctl daemon-reload
            sudo systemctl restart kubelet

    Uncordon the node:
        Bring the node back online by marking it schedulable:
        # replace <node-to-uncordon> with the name of your node
            kubectl uncordon <node-to-uncordon>
                # node comes back online




# =====================================================================================================
# ================= Update AWS EKS cluster ============================================================

Note: Refer to AWS EKS update documentaion. To be done via Web + CLI.



















































